{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e72d0a0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n",
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 39274 bytes to train1.py\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0313 02:46:48.905613 2179337 site-packages/torch/distributed/run.py:792] \n",
      "W0313 02:46:48.905613 2179337 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0313 02:46:48.905613 2179337 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0313 02:46:48.905613 2179337 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "torch.Size([2, 1])\n",
      "[rank0]:[W313 02:46:56.229345035 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank1]:[W313 02:46:56.314972918 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/1286 00:00<? [rank0]:[W313 02:46:58.356341723 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank1]:[W313 02:46:58.358552155 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "1286/1286 02:09<00:00 , loss=0.131, lr=0.000296, step=1286\n",
      "430/430 00:18<00:00 \n",
      "EPOCH 1 | AUC 0.8256570248968247\n",
      "SAVING BEST!\n",
      "1286/1286 02:12<00:00 , loss=0.0516, lr=0.000283, step=2572\n",
      "430/430 00:18<00:00 \n",
      "EPOCH 2 | AUC 0.832155731535727\n",
      "SAVING BEST!\n",
      "1286/1286 02:12<00:00 , loss=0.0342, lr=0.000262, step=3858\n",
      "430/430 00:18<00:00 \n",
      "EPOCH 3 | AUC 0.824439828972192\n",
      "1286/1286 02:12<00:00 , loss=0.0263, lr=0.000235, step=5144\n",
      "430/430 00:18<00:00 \n",
      "EPOCH 4 | AUC 0.8141273821407761\n",
      "1286/1286 02:12<00:00 , loss=0.0199, lr=0.000203, step=6430\n",
      "430/430 00:18<00:00 \n",
      "EPOCH 5 | AUC 0.83073870642811\n",
      "1286/1286 02:11<00:00 , loss=0.0147, lr=0.000168, step=7716\n",
      "430/430 00:18<00:00 \n",
      "EPOCH 6 | AUC 0.8426158976325019\n",
      "SAVING BEST!\n",
      "1286/1286 02:12<00:00 , loss=0.0102, lr=0.000132, step=9002 \n",
      "430/430 00:18<00:00 \n",
      "EPOCH 7 | AUC 0.8288721832883894\n",
      "1286/1286 02:11<00:00 , loss=0.00759, lr=9.68e-5, step=10288 \n",
      "430/430 00:18<00:00 \n",
      "EPOCH 8 | AUC 0.8200264600751925\n",
      "1286/1286 02:12<00:00 , loss=0.00506, lr=6.48e-5, step=11574\n",
      "430/430 00:18<00:00 \n",
      "EPOCH 9 | AUC 0.8314664220158967\n",
      "1286/1286 02:12<00:00 , loss=0.00266, lr=3.77e-5, step=12860\n",
      "430/430 00:19<00:00 \n",
      "EPOCH 10 | AUC 0.8390691178419089\n",
      "[rank0]:[W313 03:12:19.090108536 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#Adding this line solved my problem\n",
    "!kill $(ps aux | grep \"run.py\" | grep -v grep | awk '{print $2}')\n",
    "\n",
    "DIR = \"./data/classification_stage1/\"\n",
    "NAME = \"tf_efficientnetv2_s.in21k_ft_in1k\"\n",
    "V = \"1\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 1):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !CUDA_VISIBLE_DEVICES=1,2 python -m torch.distributed.launch --nproc_per_node=2 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b86820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36257d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n",
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 39288 bytes to train1.py\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0313 03:12:34.454163 2183093 site-packages/torch/distributed/run.py:792] \n",
      "W0313 03:12:34.454163 2183093 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0313 03:12:34.454163 2183093 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0313 03:12:34.454163 2183093 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "[rank1]:[W313 03:12:42.037060307 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "torch.Size([2, 1])\n",
      "[rank0]:[W313 03:12:42.391446995 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/1286 00:00<? [rank1]:[W313 03:12:44.819820651 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W313 03:12:44.855177200 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "1286/1286 03:43<00:00 , loss=0.127, lr=0.000296, step=1286\n",
      "430/430 00:33<00:00 \n",
      "EPOCH 1 | AUC 0.8519738036974469\n",
      "SAVING BEST!\n",
      "1286/1286 03:44<00:00 , loss=0.0559, lr=0.000283, step=2572\n",
      "430/430 00:33<00:00 \n",
      "EPOCH 2 | AUC 0.8524349117721457\n",
      "SAVING BEST!\n",
      "1286/1286 03:44<00:00 , loss=0.0341, lr=0.000262, step=3858\n",
      "430/430 00:33<00:00 \n",
      "EPOCH 3 | AUC 0.8665952584350783\n",
      "SAVING BEST!\n",
      "1286/1286 03:44<00:00 , loss=0.0242, lr=0.000235, step=5144\n",
      "430/430 00:33<00:00 \n",
      "EPOCH 4 | AUC 0.8530338935137715\n",
      "1286/1286 03:44<00:00 , loss=0.0177, lr=0.000203, step=6430\n",
      "430/430 00:33<00:00 \n",
      "EPOCH 5 | AUC 0.867316527920188\n",
      "SAVING BEST!\n",
      "1286/1286 03:44<00:00 , loss=0.0137, lr=0.000168, step=7716\n",
      "430/430 00:33<00:00 \n",
      "EPOCH 6 | AUC 0.8661080365867614\n",
      "1286/1286 03:44<00:00 , loss=0.00993, lr=0.000132, step=9002\n",
      "430/430 00:33<00:00 \n",
      "EPOCH 7 | AUC 0.8695018037795053\n",
      "SAVING BEST!\n",
      "1286/1286 03:44<00:00 , loss=0.00715, lr=9.68e-5, step=10288 \n",
      "430/430 00:32<00:00 \n",
      "EPOCH 8 | AUC 0.865392225115597\n",
      "1286/1286 03:44<00:00 , loss=0.00438, lr=6.48e-5, step=11574\n",
      "430/430 00:33<00:00 \n",
      "EPOCH 9 | AUC 0.8597550495098326\n",
      "1286/1286 03:44<00:00 , loss=0.00269, lr=3.77e-5, step=12860\n",
      "430/430 00:33<00:00 \n",
      "EPOCH 10 | AUC 0.8628705873839325\n",
      "[rank0]:[W313 03:55:57.714820605 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#Adding this line solved my problem\n",
    "!kill $(ps aux | grep \"run.py\" | grep -v grep | awk '{print $2}')\n",
    "\n",
    "DIR = \"./data/classification_stage1/\"\n",
    "NAME = \"tf_efficientnetv2_s.in21k_ft_in1k\"\n",
    "V = \"1\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 1):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !CUDA_VISIBLE_DEVICES=1,2 python -m torch.distributed.launch --nproc_per_node=2 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e1f062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78070422",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n",
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 39288 bytes to train1.py\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0313 05:24:40.899147 2201623 site-packages/torch/distributed/run.py:792] \n",
      "W0313 05:24:40.899147 2201623 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0313 05:24:40.899147 2201623 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0313 05:24:40.899147 2201623 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "torch.Size([2, 1])\n",
      "[rank1]:[W313 05:24:49.308078763 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank0]:[W313 05:24:49.451695236 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/1286 00:00<? [rank0]:[W313 05:24:52.958236351 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank1]:[W313 05:24:52.041702949 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "1286/1286 05:57<00:00 , loss=0.13, lr=0.000296, step=1286 \n",
      "430/430 00:52<00:00 \n",
      "EPOCH 1 | AUC 0.8744818180380961\n",
      "SAVING BEST!\n",
      "1286/1286 05:57<00:00 , loss=0.0588, lr=0.000283, step=2572\n",
      "430/430 00:51<00:00 \n",
      "EPOCH 2 | AUC 0.8502322502563384\n",
      "1286/1286 05:57<00:00 , loss=0.035, lr=0.000262, step=3858 \n",
      "430/430 00:51<00:00 \n",
      "EPOCH 3 | AUC 0.8799866603316064\n",
      "SAVING BEST!\n",
      "1286/1286 05:57<00:00 , loss=0.026, lr=0.000235, step=5144 \n",
      "430/430 00:52<00:00 \n",
      "EPOCH 4 | AUC 0.8804104092418856\n",
      "SAVING BEST!\n",
      "663/1286 03:04<02:54 , loss=0.0181, lr=0.000219, step=5807"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#Adding this line solved my problem\n",
    "!kill $(ps aux | grep \"run.py\" | grep -v grep | awk '{print $2}')\n",
    "\n",
    "DIR = \"./data/classification_stage1/\"\n",
    "NAME = \"tf_efficientnetv2_s.in21k_ft_in1k\"\n",
    "V = \"2\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 1):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !CUDA_VISIBLE_DEVICES=1,2 python -m torch.distributed.launch --nproc_per_node=2 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c2898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc7b4e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n",
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 39473 bytes to train1.py\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0313 06:38:36.797803 2213396 site-packages/torch/distributed/run.py:792] \n",
      "W0313 06:38:36.797803 2213396 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0313 06:38:36.797803 2213396 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0313 06:38:36.797803 2213396 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "[rank0]:[W313 06:38:44.761194580 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "torch.Size([2, 1])\n",
      "[rank1]:[W313 06:38:45.215737334 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/1286 00:00<? [rank1]:[W313 06:38:47.832552525 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W313 06:38:47.891572375 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "1286/1286 05:57<00:00 , loss=0.142, lr=0.000296, step=1286\n",
      "430/430 00:52<00:00 \n",
      "EPOCH 1 | AUC 0.8527697326965902\n",
      "SAVING BEST!\n",
      "1286/1286 05:58<00:00 , loss=0.073, lr=0.000283, step=2572 \n",
      "430/430 00:52<00:00 \n",
      "EPOCH 2 | AUC 0.8769432999867502\n",
      "SAVING BEST!\n",
      "1286/1286 05:59<00:00 , loss=0.0463, lr=0.000262, step=3858\n",
      "430/430 00:52<00:00 \n",
      "EPOCH 3 | AUC 0.884407169345987\n",
      "SAVING BEST!\n",
      "1286/1286 05:58<00:00 , loss=0.0362, lr=0.000235, step=5144\n",
      "430/430 00:52<00:00 \n",
      "EPOCH 4 | AUC 0.8909944863707745\n",
      "SAVING BEST!\n",
      "1286/1286 05:58<00:00 , loss=0.0259, lr=0.000203, step=6430\n",
      "430/430 00:52<00:00 \n",
      "EPOCH 5 | AUC 0.8476919682313476\n",
      "1286/1286 05:58<00:00 , loss=0.0178, lr=0.000168, step=7716\n",
      "430/430 00:52<00:00 \n",
      "EPOCH 6 | AUC 0.8717873471191473\n",
      "1129/1286 05:14<00:43 , loss=0.014, lr=0.000136, step=8845 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x74f21c28e1f0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harshit/anaconda3/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#Adding this line solved my problem\n",
    "!kill $(ps aux | grep \"run.py\" | grep -v grep | awk '{print $2}')\n",
    "\n",
    "DIR = \"./data/classification_stage1/\"\n",
    "NAME = \"tf_efficientnetv2_s.in21k_ft_in1k\"\n",
    "V = \"3\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 1):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !CUDA_VISIBLE_DEVICES=1,2 python -m torch.distributed.launch --nproc_per_node=2 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb401e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ed1054",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n",
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 39641 bytes to train1.py\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0313 08:41:14.468998 2229584 site-packages/torch/distributed/run.py:792] \n",
      "W0313 08:41:14.468998 2229584 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0313 08:41:14.468998 2229584 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0313 08:41:14.468998 2229584 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "torch.Size([2, 1])\n",
      "[rank1]:[W313 08:41:22.754680291 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank0]:[W313 08:41:23.083714086 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/1290 00:00<? [rank1]:[W313 08:41:26.158228001 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W313 08:41:26.174986231 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "1290/1290 06:02<00:00 , loss=0.153, lr=0.000296, step=1290\n",
      "426/426 00:51<00:00 \n",
      "EPOCH 1 | AUC 0.848884547435156\n",
      "SAVING BEST!\n",
      "1290/1290 06:04<00:00 , loss=0.0826, lr=0.000283, step=2580\n",
      "426/426 00:52<00:00 \n",
      "EPOCH 2 | AUC 0.8288537102892005\n",
      "1290/1290 06:01<00:00 , loss=0.0534, lr=0.000262, step=3870\n",
      "426/426 00:52<00:00 \n",
      "EPOCH 3 | AUC 0.8570414638299453\n",
      "SAVING BEST!\n",
      "1290/1290 06:01<00:00 , loss=0.031, lr=0.000235, step=5160 \n",
      "426/426 00:51<00:00 \n",
      "EPOCH 4 | AUC 0.8473237835024334\n",
      "1290/1290 06:01<00:00 , loss=0.0226, lr=0.000203, step=6450\n",
      "426/426 00:52<00:00 \n",
      "EPOCH 5 | AUC 0.838597687812842\n",
      "1290/1290 06:02<00:00 , loss=0.0161, lr=0.000168, step=7740\n",
      "426/426 00:51<00:00 \n",
      "EPOCH 6 | AUC 0.8415874468053456\n",
      "1290/1290 06:02<00:00 , loss=0.00962, lr=0.000132, step=9030\n",
      "426/426 00:52<00:00 \n",
      "EPOCH 7 | AUC 0.8260109609919581\n",
      "1290/1290 06:01<00:00 , loss=0.00666, lr=9.68e-5, step=10320 \n",
      "426/426 00:52<00:00 \n",
      "EPOCH 8 | AUC 0.831576642912908\n",
      "1290/1290 06:01<00:00 , loss=0.0034, lr=6.48e-5, step=11610 \n",
      "426/426 00:52<00:00 \n",
      "EPOCH 9 | AUC 0.83409401892375\n",
      "1290/1290 06:00<00:00 , loss=0.0016, lr=3.77e-5, step=12900 \n",
      "426/426 00:52<00:00 \n",
      "EPOCH 10 | AUC 0.8393726476769086\n",
      "[rank0]:[W313 09:50:39.405819742 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0313 09:50:42.765588 2239919 site-packages/torch/distributed/run.py:792] \n",
      "W0313 09:50:42.765588 2239919 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0313 09:50:42.765588 2239919 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0313 09:50:42.765588 2239919 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "torch.Size([2, 1])\n",
      "[rank0]:[W313 09:50:51.097981991 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank1]:[W313 09:50:51.123457287 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/1302 00:00<? [rank0]:[W313 09:50:53.802161900 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank1]:[W313 09:50:53.811719591 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "1302/1302 06:09<00:00 , loss=0.152, lr=0.000296, step=1302\n",
      "420/420 00:52<00:00 \n",
      "EPOCH 1 | AUC 0.8229753042930306\n",
      "SAVING BEST!\n",
      "1302/1302 06:10<00:00 , loss=0.0856, lr=0.000283, step=2604\n",
      "420/420 00:52<00:00 \n",
      "EPOCH 2 | AUC 0.8058089798620025\n",
      "1302/1302 06:10<00:00 , loss=0.051, lr=0.000262, step=3906 \n",
      "420/420 00:51<00:00 \n",
      "EPOCH 3 | AUC 0.7957528306522785\n",
      "1302/1302 06:07<00:00 , loss=0.0335, lr=0.000235, step=5208\n",
      "420/420 00:52<00:00 \n",
      "EPOCH 4 | AUC 0.8045940015578198\n",
      "1302/1302 06:08<00:00 , loss=0.0214, lr=0.000203, step=6510\n",
      "420/420 00:50<00:00 \n",
      "EPOCH 5 | AUC 0.8195963228264067\n",
      "1302/1302 06:05<00:00 , loss=0.0152, lr=0.000168, step=7812\n",
      "420/420 00:50<00:00 \n",
      "EPOCH 6 | AUC 0.8261303909425812\n",
      "SAVING BEST!\n",
      "1302/1302 06:06<00:00 , loss=0.01, lr=0.000132, step=9114   \n",
      "420/420 00:52<00:00 \n",
      "EPOCH 7 | AUC 0.808908855420277\n",
      "1302/1302 06:08<00:00 , loss=0.00478, lr=9.68e-5, step=10416 \n",
      "420/420 00:51<00:00 \n",
      "EPOCH 8 | AUC 0.8355978656029364\n",
      "SAVING BEST!\n",
      "1302/1302 06:04<00:00 , loss=0.0039, lr=6.48e-5, step=11718 \n",
      "420/420 00:51<00:00 \n",
      "EPOCH 9 | AUC 0.8219300021562099\n",
      "1302/1302 06:04<00:00 , loss=0.00179, lr=3.77e-5, step=13020\n",
      "420/420 00:52<00:00 \n",
      "EPOCH 10 | AUC 0.8225489722901959\n",
      "[rank0]:[W313 11:01:02.241969898 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0313 11:01:05.604015 2248892 site-packages/torch/distributed/run.py:792] \n",
      "W0313 11:01:05.604015 2248892 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0313 11:01:05.604015 2248892 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0313 11:01:05.604015 2248892 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "[rank0]:[W313 11:01:13.758907557 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "torch.Size([2, 1])\n",
      "[rank1]:[W313 11:01:14.174265304 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/1303 00:00<? [rank1]:[W313 11:01:16.922773511 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W313 11:01:16.930965923 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "1303/1303 06:05<00:00 , loss=0.156, lr=0.000296, step=1303\n",
      "425/425 00:53<00:00 \n",
      "EPOCH 1 | AUC 0.8725100652964783\n",
      "SAVING BEST!\n",
      "1303/1303 06:06<00:00 , loss=0.0842, lr=0.000283, step=2606\n",
      "425/425 00:53<00:00 \n",
      "EPOCH 2 | AUC 0.8602871061846129\n",
      "1303/1303 06:05<00:00 , loss=0.0521, lr=0.000262, step=3909\n",
      "425/425 00:52<00:00 \n",
      "EPOCH 3 | AUC 0.8617498688525989\n",
      "1303/1303 06:04<00:00 , loss=0.0301, lr=0.000235, step=5212\n",
      "425/425 00:52<00:00 \n",
      "EPOCH 4 | AUC 0.8556402019030235\n",
      "1303/1303 06:03<00:00 , loss=0.0218, lr=0.000203, step=6515\n",
      "425/425 00:52<00:00 \n",
      "EPOCH 5 | AUC 0.8387365920441507\n",
      "1303/1303 06:02<00:00 , loss=0.0135, lr=0.000168, step=7818\n",
      "425/425 00:52<00:00 \n",
      "EPOCH 6 | AUC 0.856543839484111\n",
      "538/1303 02:30<03:34 , loss=0.0124, lr=0.000153, step=8356"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#Adding this line solved my problem\n",
    "!kill $(ps aux | grep \"run.py\" | grep -v grep | awk '{print $2}')\n",
    "\n",
    "DIR = \"./data/classification_stage1/\"\n",
    "NAME = \"tf_efficientnetv2_s.in21k_ft_in1k\"\n",
    "V = \"4\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 4):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !CUDA_VISIBLE_DEVICES=1,2 python -m torch.distributed.launch --nproc_per_node=2 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de41fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1f1132c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n",
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 39642 bytes to train1.py\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0313 15:48:39.052688 2284561 site-packages/torch/distributed/run.py:792] \n",
      "W0313 15:48:39.052688 2284561 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0313 15:48:39.052688 2284561 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0313 15:48:39.052688 2284561 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "torch.Size([2, 1])\n",
      "[rank0]:[W313 15:48:47.161066337 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank1]:[W313 15:48:47.352300218 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/1290 00:00<? [rank1]:[W313 15:48:49.594196358 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W313 15:48:49.648284297 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "1290/1290 06:00<00:00 , loss=0.143, lr=0.000296, step=1290\n",
      "426/426 00:52<00:00 \n",
      "EPOCH 1 | AUC 0.8665270192433475\n",
      "SAVING BEST!\n",
      "1290/1290 06:02<00:00 , loss=0.0719, lr=0.000283, step=2580\n",
      "426/426 00:51<00:00 \n",
      "EPOCH 2 | AUC 0.8559201495010335\n",
      "1290/1290 06:02<00:00 , loss=0.0467, lr=0.000262, step=3870\n",
      "426/426 00:51<00:00 \n",
      "EPOCH 3 | AUC 0.8540340476434469\n",
      "1290/1290 06:01<00:00 , loss=0.0328, lr=0.000235, step=5160\n",
      "426/426 00:51<00:00 \n",
      "EPOCH 4 | AUC 0.8675850364215577\n",
      "SAVING BEST!\n",
      "1290/1290 06:01<00:00 , loss=0.025, lr=0.000203, step=6450 \n",
      "426/426 00:52<00:00 \n",
      "EPOCH 5 | AUC 0.8577886531005197\n",
      "1290/1290 06:02<00:00 , loss=0.0187, lr=0.000168, step=7740\n",
      "426/426 00:50<00:00 \n",
      "EPOCH 6 | AUC 0.84412765458172\n",
      "1290/1290 06:01<00:00 , loss=0.0152, lr=0.000132, step=9030\n",
      "426/426 00:51<00:00 \n",
      "EPOCH 7 | AUC 0.8533512870927491\n",
      "1290/1290 06:01<00:00 , loss=0.00977, lr=9.68e-5, step=10320 \n",
      "426/426 00:52<00:00 \n",
      "EPOCH 8 | AUC 0.8434409571132689\n",
      "1290/1290 06:01<00:00 , loss=0.00655, lr=6.48e-5, step=11610\n",
      "426/426 00:51<00:00 \n",
      "EPOCH 9 | AUC 0.8502472985495099\n",
      "1290/1290 06:00<00:00 , loss=0.00438, lr=3.77e-5, step=12900\n",
      "426/426 00:52<00:00 \n",
      "EPOCH 10 | AUC 0.8466002393127358\n",
      "[rank0]:[W313 16:57:58.243828770 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0313 16:58:01.524713 2293431 site-packages/torch/distributed/run.py:792] \n",
      "W0313 16:58:01.524713 2293431 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0313 16:58:01.524713 2293431 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0313 16:58:01.524713 2293431 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "torch.Size([2, 1])\n",
      "[rank1]:[W313 16:58:09.634307317 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank0]:[W313 16:58:09.898978707 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/1302 00:00<? [rank1]:[W313 16:58:12.103012314 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W313 16:58:12.146376622 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "1302/1302 06:04<00:00 , loss=0.138, lr=0.000296, step=1302\n",
      "420/420 00:51<00:00 \n",
      "EPOCH 1 | AUC 0.8550347918672444\n",
      "SAVING BEST!\n",
      "1302/1302 06:05<00:00 , loss=0.0725, lr=0.000283, step=2604\n",
      "420/420 00:51<00:00 \n",
      "EPOCH 2 | AUC 0.8683378860284285\n",
      "SAVING BEST!\n",
      "1302/1302 06:05<00:00 , loss=0.0456, lr=0.000262, step=3906\n",
      "420/420 00:50<00:00 \n",
      "EPOCH 3 | AUC 0.832760861698358\n",
      "1302/1302 06:04<00:00 , loss=0.0321, lr=0.000235, step=5208\n",
      "420/420 00:50<00:00 \n",
      "EPOCH 4 | AUC 0.8394942120807927\n",
      "1302/1302 06:04<00:00 , loss=0.0237, lr=0.000203, step=6510\n",
      "420/420 00:51<00:00 \n",
      "EPOCH 5 | AUC 0.8515598151843978\n",
      "1302/1302 06:05<00:00 , loss=0.0182, lr=0.000168, step=7812\n",
      "420/420 00:51<00:00 \n",
      "EPOCH 6 | AUC 0.852251546955231\n",
      "1302/1302 06:05<00:00 , loss=0.013, lr=0.000132, step=9114 \n",
      "420/420 00:51<00:00 \n",
      "EPOCH 7 | AUC 0.8461785776369277\n",
      "1302/1302 06:05<00:00 , loss=0.00995, lr=9.68e-5, step=10416 \n",
      "420/420 00:51<00:00 \n",
      "EPOCH 8 | AUC 0.8383777988941485\n",
      "1302/1302 06:04<00:00 , loss=0.00665, lr=6.48e-5, step=11718\n",
      "294/420 00:36<00:15 "
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#Adding this line solved my problem\n",
    "!kill $(ps aux | grep \"run.py\" | grep -v grep | awk '{print $2}')\n",
    "\n",
    "DIR = \"./data/classification_stage1/\"\n",
    "NAME = \"tf_efficientnetv2_s.in21k_ft_in1k\"\n",
    "V = \"5\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 4):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !CUDA_VISIBLE_DEVICES=1,2 python -m torch.distributed.launch --nproc_per_node=2 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f84aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b00eef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n",
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 39677 bytes to train1.py\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0313 21:05:29.721094 2324893 site-packages/torch/distributed/run.py:792] \n",
      "W0313 21:05:29.721094 2324893 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0313 21:05:29.721094 2324893 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0313 21:05:29.721094 2324893 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "[rank0]:[W313 21:05:37.823346002 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "torch.Size([2, 1])\n",
      "[rank1]:[W313 21:05:38.706075621 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/1290 00:00<? [rank1]:[W313 21:05:40.851768627 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W313 21:05:40.906386373 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "1290/1290 05:56<00:00 , loss=0.154, lr=0.000296, step=1290\n",
      "426/426 00:51<00:00 \n",
      "EPOCH 1 | AUC 0.8620601027780708\n",
      "SAVING BEST!\n",
      "1290/1290 06:00<00:00 , loss=0.0905, lr=0.000283, step=2580\n",
      "426/426 00:51<00:00 \n",
      "EPOCH 2 | AUC 0.8688454866102244\n",
      "SAVING BEST!\n",
      "1290/1290 05:58<00:00 , loss=0.0648, lr=0.000262, step=3870\n",
      "426/426 00:51<00:00 \n",
      "EPOCH 3 | AUC 0.8718704774804651\n",
      "SAVING BEST!\n",
      "1290/1290 05:56<00:00 , loss=0.0475, lr=0.000235, step=5160\n",
      "426/426 00:52<00:00 \n",
      "EPOCH 4 | AUC 0.8680053082852952\n",
      "1290/1290 05:58<00:00 , loss=0.0366, lr=0.000203, step=6450\n",
      "426/426 00:51<00:00 \n",
      "EPOCH 5 | AUC 0.8653895329049826\n",
      "1290/1290 05:57<00:00 , loss=0.0276, lr=0.000168, step=7740\n",
      "426/426 00:51<00:00 \n",
      "EPOCH 6 | AUC 0.8416746247805021\n",
      "1290/1290 05:57<00:00 , loss=0.0209, lr=0.000132, step=9030\n",
      "426/426 00:51<00:00 \n",
      "EPOCH 7 | AUC 0.8479769818084459\n",
      "1290/1290 05:57<00:00 , loss=0.016, lr=9.68e-5, step=10320  \n",
      "22/426 00:03<00:49 "
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#Adding this line solved my problem\n",
    "!kill $(ps aux | grep \"run.py\" | grep -v grep | awk '{print $2}')\n",
    "\n",
    "DIR = \"./data/classification_stage1/\"\n",
    "NAME = \"tf_efficientnetv2_s.in21k_ft_in1k\"\n",
    "V = \"6\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 4):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !CUDA_VISIBLE_DEVICES=1,2 python -m torch.distributed.launch --nproc_per_node=2 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068cc909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3755e4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec464a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e782c41b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n",
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 39939 bytes to train1.py\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0314 02:53:52.288700 2371313 site-packages/torch/distributed/run.py:792] \n",
      "W0314 02:53:52.288700 2371313 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0314 02:53:52.288700 2371313 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0314 02:53:52.288700 2371313 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "torch.Size([2, 1])\n",
      "[rank1]:[W314 02:54:00.395025949 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank0]:[W314 02:54:00.547901172 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/1290 00:00<? [rank1]:[W314 02:54:02.874451134 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W314 02:54:02.875269170 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "1290/1290 05:52<00:00 , loss=0.181, lr=0.000985, step=1290\n",
      "426/426 00:49<00:00 \n",
      "EPOCH 1 | AUC 0.8364543307757277\n",
      "SAVING BEST!\n",
      "1290/1290 05:54<00:00 , loss=0.129, lr=0.000943, step=2580\n",
      "426/426 00:48<00:00 \n",
      "EPOCH 2 | AUC 0.8692392491086123\n",
      "SAVING BEST!\n",
      "1290/1290 05:52<00:00 , loss=0.103, lr=0.000874, step=3870\n",
      "426/426 00:49<00:00 \n",
      "EPOCH 3 | AUC 0.8757760649120886\n",
      "SAVING BEST!\n",
      "1290/1290 05:52<00:00 , loss=0.0846, lr=0.000784, step=5160\n",
      "426/426 00:49<00:00 \n",
      "EPOCH 4 | AUC 0.8837104865179308\n",
      "SAVING BEST!\n",
      "1290/1290 05:52<00:00 , loss=0.0694, lr=0.000677, step=6450\n",
      "426/426 00:49<00:00 \n",
      "EPOCH 5 | AUC 0.8784447647256696\n",
      "1290/1290 05:53<00:00 , loss=0.0551, lr=0.00056, step=7740 \n",
      "426/426 00:48<00:00 \n",
      "EPOCH 6 | AUC 0.8684794475566133\n",
      "1290/1290 05:53<00:00 , loss=0.0406, lr=0.00044, step=9030 \n",
      "426/426 00:48<00:00 \n",
      "EPOCH 7 | AUC 0.8546707321118185\n",
      "1290/1290 05:53<00:00 , loss=0.0303, lr=0.000323, step=10320\n",
      "426/426 00:48<00:00 \n",
      "EPOCH 8 | AUC 0.8632853210890004\n",
      "1290/1290 05:52<00:00 , loss=0.0229, lr=0.000216, step=11610\n",
      "426/426 00:48<00:00 \n",
      "EPOCH 9 | AUC 0.8435948269466508\n",
      "1290/1290 05:53<00:00 , loss=0.0186, lr=0.000126, step=12900\n",
      "426/426 00:49<00:00 \n",
      "EPOCH 10 | AUC 0.864496076531418\n",
      "[rank0]:[W314 04:01:12.726082753 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0314 04:01:15.971577 2379948 site-packages/torch/distributed/run.py:792] \n",
      "W0314 04:01:15.971577 2379948 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0314 04:01:15.971577 2379948 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0314 04:01:15.971577 2379948 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "[rank1]:[W314 04:01:23.906407039 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "torch.Size([2, 1])\n",
      "[rank0]:[W314 04:01:24.369263934 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/1302 00:00<? [rank1]:[W314 04:01:26.705356665 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W314 04:01:26.739832219 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "1302/1302 05:53<00:00 , loss=0.179, lr=0.000985, step=1302\n",
      "420/420 00:49<00:00 \n",
      "EPOCH 1 | AUC 0.7647784368982671\n",
      "SAVING BEST!\n",
      "1302/1302 05:55<00:00 , loss=0.128, lr=0.000943, step=2604\n",
      "420/420 00:48<00:00 \n",
      "EPOCH 2 | AUC 0.8208520643620293\n",
      "SAVING BEST!\n",
      "1302/1302 05:55<00:00 , loss=0.0996, lr=0.000874, step=3906\n",
      "420/420 00:48<00:00 \n",
      "EPOCH 3 | AUC 0.8126350974138853\n",
      "1302/1302 05:53<00:00 , loss=0.0814, lr=0.000784, step=5208\n",
      "420/420 00:48<00:00 \n",
      "EPOCH 4 | AUC 0.8423359274711169\n",
      "SAVING BEST!\n",
      "1302/1302 05:53<00:00 , loss=0.0661, lr=0.000677, step=6510\n",
      "420/420 00:47<00:00 \n",
      "EPOCH 5 | AUC 0.8211952297617138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 05:53<00:00 , loss=0.0522, lr=0.00056, step=7812 \n",
      "420/420 00:48<00:00 \n",
      "EPOCH 6 | AUC 0.8557156067808622\n",
      "SAVING BEST!\n",
      "1302/1302 05:54<00:00 , loss=0.0439, lr=0.00044, step=9114 \n",
      "420/420 00:48<00:00 \n",
      "EPOCH 7 | AUC 0.8637233148133291\n",
      "SAVING BEST!\n",
      "1302/1302 05:54<00:00 , loss=0.0321, lr=0.000323, step=10416\n",
      "420/420 00:47<00:00 \n",
      "EPOCH 8 | AUC 0.8437416217038405\n",
      "1302/1302 05:53<00:00 , loss=0.0245, lr=0.000216, step=11718\n",
      "420/420 00:48<00:00 \n",
      "EPOCH 9 | AUC 0.8243307183020432\n",
      "1302/1302 05:53<00:00 , loss=0.0172, lr=0.000126, step=13020\n",
      "420/420 00:48<00:00 \n",
      "EPOCH 10 | AUC 0.843277409773481\n",
      "[rank0]:[W314 05:08:41.538100732 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0314 05:08:44.754688 2388588 site-packages/torch/distributed/run.py:792] \n",
      "W0314 05:08:44.754688 2388588 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0314 05:08:44.754688 2388588 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0314 05:08:44.754688 2388588 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "[rank1]:[W314 05:08:52.640798744 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "torch.Size([2, 1])\n",
      "[rank0]:[W314 05:08:53.165066018 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/1303 00:00<? [rank1]:[W314 05:08:55.488128953 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W314 05:08:55.522641271 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "1303/1303 06:01<00:00 , loss=0.183, lr=0.000985, step=1303\n",
      "425/425 00:50<00:00 \n",
      "EPOCH 1 | AUC 0.8602173954765875\n",
      "SAVING BEST!\n",
      "1303/1303 06:01<00:00 , loss=0.13, lr=0.000943, step=2606 \n",
      "425/425 00:49<00:00 \n",
      "EPOCH 2 | AUC 0.872504958011507\n",
      "SAVING BEST!\n",
      "1303/1303 06:00<00:00 , loss=0.103, lr=0.000874, step=3909\n",
      "425/425 00:49<00:00 \n",
      "EPOCH 3 | AUC 0.868736943903032\n",
      "1303/1303 06:00<00:00 , loss=0.0795, lr=0.000784, step=5212\n",
      "425/425 00:49<00:00 \n",
      "EPOCH 4 | AUC 0.878757586290725\n",
      "SAVING BEST!\n",
      "1303/1303 06:00<00:00 , loss=0.0667, lr=0.000677, step=6515\n",
      "425/425 00:49<00:00 \n",
      "EPOCH 5 | AUC 0.8841469661234533\n",
      "SAVING BEST!\n",
      "1303/1303 06:00<00:00 , loss=0.0535, lr=0.00056, step=7818 \n",
      "425/425 00:49<00:00 \n",
      "EPOCH 6 | AUC 0.8959982449705505\n",
      "SAVING BEST!\n",
      "1303/1303 05:59<00:00 , loss=0.0397, lr=0.00044, step=9121 \n",
      "425/425 00:49<00:00 \n",
      "EPOCH 7 | AUC 0.8708361872998136\n",
      "1303/1303 06:01<00:00 , loss=0.0284, lr=0.000323, step=10424\n",
      "425/425 00:49<00:00 \n",
      "EPOCH 8 | AUC 0.8648009118476215\n",
      "1303/1303 05:58<00:00 , loss=0.023, lr=0.000216, step=11727 \n",
      "425/425 00:49<00:00 \n",
      "EPOCH 9 | AUC 0.8698920113106637\n",
      "1303/1303 05:58<00:00 , loss=0.0153, lr=0.000126, step=13030\n",
      "425/425 00:49<00:00 \n",
      "EPOCH 10 | AUC 0.8646818449347249\n",
      "[rank0]:[W314 06:17:24.201609743 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0314 06:17:27.530615 2397523 site-packages/torch/distributed/run.py:792] \n",
      "W0314 06:17:27.530615 2397523 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0314 06:17:27.530615 2397523 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0314 06:17:27.530615 2397523 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "[rank0]:[W314 06:17:35.480071729 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "torch.Size([2, 1])\n",
      "[rank1]:[W314 06:17:37.956025497 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/1286 00:00<? [rank1]:[W314 06:17:39.198301317 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W314 06:17:39.216700744 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286/1286 05:51<00:00 , loss=0.195, lr=0.000985, step=1286\n",
      "413/413 00:48<00:00 \n",
      "EPOCH 1 | AUC 0.8206277757712495\n",
      "SAVING BEST!\n",
      "1286/1286 05:53<00:00 , loss=0.134, lr=0.000943, step=2572\n",
      "413/413 00:47<00:00 \n",
      "EPOCH 2 | AUC 0.8591668785507358\n",
      "SAVING BEST!\n",
      "1286/1286 05:51<00:00 , loss=0.108, lr=0.000874, step=3858\n",
      "413/413 00:47<00:00 \n",
      "EPOCH 3 | AUC 0.8416077548914417\n",
      "1286/1286 05:49<00:00 , loss=0.0854, lr=0.000784, step=5144\n",
      "413/413 00:47<00:00 \n",
      "EPOCH 4 | AUC 0.863288885743338\n",
      "SAVING BEST!\n",
      "1286/1286 05:51<00:00 , loss=0.069, lr=0.000677, step=6430 \n",
      "413/413 00:47<00:00 \n",
      "EPOCH 5 | AUC 0.8405179597124929\n",
      "1286/1286 05:51<00:00 , loss=0.0532, lr=0.00056, step=7716 \n",
      "413/413 00:47<00:00 \n",
      "EPOCH 6 | AUC 0.8497558423671783\n",
      "1286/1286 05:51<00:00 , loss=0.0403, lr=0.00044, step=9002 \n",
      "413/413 00:47<00:00 \n",
      "EPOCH 7 | AUC 0.8315900749920438\n",
      "160/1286 00:44<05:04 , loss=0.0302, lr=0.000425, step=9162^C\n",
      "W0314 07:05:04.813325 2397523 site-packages/torch/distributed/elastic/agent/server/api.py:719] Received Signals.SIGINT death signal, shutting down workers\n",
      "W0314 07:05:04.814641 2397523 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2397573 closing signal SIGINT\n",
      "W0314 07:05:04.814910 2397523 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2397574 closing signal SIGINT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#Adding this line solved my problem\n",
    "!kill $(ps aux | grep \"run.py\" | grep -v grep | awk '{print $2}')\n",
    "\n",
    "DIR = \"./data/classification_stage1/\"\n",
    "NAME = \"tf_efficientnet_b3.ns_jft_in1k\"\n",
    "V = \"1\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 4):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !CUDA_VISIBLE_DEVICES=1,2 python -m torch.distributed.launch --nproc_per_node=2 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f160b2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbd659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de7e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de38c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b2dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eac1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df6b5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c20b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.8570414638299453,\n",
       "  0.8355978656029364,\n",
       "  0.8725100652964783,\n",
       "  0.8579576537631352],\n",
       " 0.8557767621231238)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "scs = [np.array([float(line.split('| AUC ')[1].split(' |')[0]) for line in open(f'./data/classification_stage1/tf_efficientnetv2_s.in21k_ft_in1k_v4/log_{F}.txt').readlines() if 'AUC' in line]).max() for F in range(4)]\n",
    "scs, np.mean(scs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca43144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cfcd8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.8675850364215577,\n",
       "  0.8683378860284285,\n",
       "  0.8828426572382704,\n",
       "  0.8761409572079395],\n",
       " 0.873726634224049)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "scs = [np.array([float(line.split('| AUC ')[1].split(' |')[0]) for line in open(f'./data/classification_stage1/tf_efficientnetv2_s.in21k_ft_in1k_v5/log_{F}.txt').readlines() if 'AUC' in line]).max() for F in range(4)]\n",
    "scs, np.mean(scs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b74ee96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5dbd5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.8718704774804651,\n",
       "  0.8723636886733793,\n",
       "  0.8867102367477151,\n",
       "  0.8886824850099955],\n",
       " 0.8799067219778888)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "scs = [np.array([float(line.split('| AUC ')[1].split(' |')[0]) for line in open(f'./data/classification_stage1/tf_efficientnetv2_s.in21k_ft_in1k_v6/log_{F}.txt').readlines() if 'AUC' in line]).max() for F in range(4)]\n",
    "scs, np.mean(scs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d557d273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1122f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.8837104865179308,\n",
       "  0.8637233148133291,\n",
       "  0.8959982449705505,\n",
       "  0.863288885743338],\n",
       " 0.8766802330112872)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "scs = [np.array([float(line.split('| AUC ')[1].split(' |')[0]) for line in open(f'./data/classification_stage1/tf_efficientnet_b3.ns_jft_in1k_v1/log_{F}.txt').readlines() if 'AUC' in line]).max() for F in range(4)]\n",
    "scs, np.mean(scs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b21aa5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f63664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd289e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfd0914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b459ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
