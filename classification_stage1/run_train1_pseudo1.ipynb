{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b00eef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n",
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 39677 bytes to train1.py\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0313 21:05:29.721094 2324893 site-packages/torch/distributed/run.py:792] \n",
      "W0313 21:05:29.721094 2324893 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0313 21:05:29.721094 2324893 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0313 21:05:29.721094 2324893 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "[rank0]:[W313 21:05:37.823346002 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "torch.Size([2, 1])\n",
      "[rank1]:[W313 21:05:38.706075621 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/1290 00:00<? [rank1]:[W313 21:05:40.851768627 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W313 21:05:40.906386373 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "1290/1290 05:56<00:00 , loss=0.154, lr=0.000296, step=1290\n",
      "426/426 00:51<00:00 \n",
      "EPOCH 1 | AUC 0.8620601027780708\n",
      "SAVING BEST!\n",
      "1290/1290 06:00<00:00 , loss=0.0905, lr=0.000283, step=2580\n",
      "426/426 00:51<00:00 \n",
      "EPOCH 2 | AUC 0.8688454866102244\n",
      "SAVING BEST!\n",
      "1290/1290 05:58<00:00 , loss=0.0648, lr=0.000262, step=3870\n",
      "426/426 00:51<00:00 \n",
      "EPOCH 3 | AUC 0.8718704774804651\n",
      "SAVING BEST!\n",
      "1290/1290 05:56<00:00 , loss=0.0475, lr=0.000235, step=5160\n",
      "426/426 00:52<00:00 \n",
      "EPOCH 4 | AUC 0.8680053082852952\n",
      "1290/1290 05:58<00:00 , loss=0.0366, lr=0.000203, step=6450\n",
      "426/426 00:51<00:00 \n",
      "EPOCH 5 | AUC 0.8653895329049826\n",
      "1290/1290 05:57<00:00 , loss=0.0276, lr=0.000168, step=7740\n",
      "426/426 00:51<00:00 \n",
      "EPOCH 6 | AUC 0.8416746247805021\n",
      "1290/1290 05:57<00:00 , loss=0.0209, lr=0.000132, step=9030\n",
      "426/426 00:51<00:00 \n",
      "EPOCH 7 | AUC 0.8479769818084459\n",
      "1290/1290 05:57<00:00 , loss=0.016, lr=9.68e-5, step=10320  \n",
      "22/426 00:03<00:49 "
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#Adding this line solved my problem\n",
    "!kill $(ps aux | grep \"run.py\" | grep -v grep | awk '{print $2}')\n",
    "\n",
    "DIR = \"./data/classification_stage1/\"\n",
    "NAME = \"tf_efficientnetv2_s.in21k_ft_in1k\"\n",
    "V = \"6\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 4):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !CUDA_VISIBLE_DEVICES=1,2 python -m torch.distributed.launch --nproc_per_node=2 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068cc909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e782c41b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n",
      "[NbConvertApp] Converting notebook train1.ipynb to script\n",
      "[NbConvertApp] Writing 39939 bytes to train1.py\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0314 02:53:52.288700 2371313 site-packages/torch/distributed/run.py:792] \n",
      "W0314 02:53:52.288700 2371313 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0314 02:53:52.288700 2371313 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0314 02:53:52.288700 2371313 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "torch.Size([2, 1])\n",
      "[rank1]:[W314 02:54:00.395025949 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank0]:[W314 02:54:00.547901172 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/1290 00:00<? [rank1]:[W314 02:54:02.874451134 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W314 02:54:02.875269170 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "1290/1290 05:52<00:00 , loss=0.181, lr=0.000985, step=1290\n",
      "426/426 00:49<00:00 \n",
      "EPOCH 1 | AUC 0.8364543307757277\n",
      "SAVING BEST!\n",
      "1290/1290 05:54<00:00 , loss=0.129, lr=0.000943, step=2580\n",
      "426/426 00:48<00:00 \n",
      "EPOCH 2 | AUC 0.8692392491086123\n",
      "SAVING BEST!\n",
      "1290/1290 05:52<00:00 , loss=0.103, lr=0.000874, step=3870\n",
      "426/426 00:49<00:00 \n",
      "EPOCH 3 | AUC 0.8757760649120886\n",
      "SAVING BEST!\n",
      "1290/1290 05:52<00:00 , loss=0.0846, lr=0.000784, step=5160\n",
      "426/426 00:49<00:00 \n",
      "EPOCH 4 | AUC 0.8837104865179308\n",
      "SAVING BEST!\n",
      "1290/1290 05:52<00:00 , loss=0.0694, lr=0.000677, step=6450\n",
      "426/426 00:49<00:00 \n",
      "EPOCH 5 | AUC 0.8784447647256696\n",
      "1290/1290 05:53<00:00 , loss=0.0551, lr=0.00056, step=7740 \n",
      "426/426 00:48<00:00 \n",
      "EPOCH 6 | AUC 0.8684794475566133\n",
      "1290/1290 05:53<00:00 , loss=0.0406, lr=0.00044, step=9030 \n",
      "426/426 00:48<00:00 \n",
      "EPOCH 7 | AUC 0.8546707321118185\n",
      "1290/1290 05:53<00:00 , loss=0.0303, lr=0.000323, step=10320\n",
      "426/426 00:48<00:00 \n",
      "EPOCH 8 | AUC 0.8632853210890004\n",
      "1290/1290 05:52<00:00 , loss=0.0229, lr=0.000216, step=11610\n",
      "426/426 00:48<00:00 \n",
      "EPOCH 9 | AUC 0.8435948269466508\n",
      "1290/1290 05:53<00:00 , loss=0.0186, lr=0.000126, step=12900\n",
      "426/426 00:49<00:00 \n",
      "EPOCH 10 | AUC 0.864496076531418\n",
      "[rank0]:[W314 04:01:12.726082753 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0314 04:01:15.971577 2379948 site-packages/torch/distributed/run.py:792] \n",
      "W0314 04:01:15.971577 2379948 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0314 04:01:15.971577 2379948 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0314 04:01:15.971577 2379948 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "[rank1]:[W314 04:01:23.906407039 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "torch.Size([2, 1])\n",
      "[rank0]:[W314 04:01:24.369263934 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/1302 00:00<? [rank1]:[W314 04:01:26.705356665 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W314 04:01:26.739832219 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "1302/1302 05:53<00:00 , loss=0.179, lr=0.000985, step=1302\n",
      "420/420 00:49<00:00 \n",
      "EPOCH 1 | AUC 0.7647784368982671\n",
      "SAVING BEST!\n",
      "1302/1302 05:55<00:00 , loss=0.128, lr=0.000943, step=2604\n",
      "420/420 00:48<00:00 \n",
      "EPOCH 2 | AUC 0.8208520643620293\n",
      "SAVING BEST!\n",
      "1302/1302 05:55<00:00 , loss=0.0996, lr=0.000874, step=3906\n",
      "420/420 00:48<00:00 \n",
      "EPOCH 3 | AUC 0.8126350974138853\n",
      "1302/1302 05:53<00:00 , loss=0.0814, lr=0.000784, step=5208\n",
      "420/420 00:48<00:00 \n",
      "EPOCH 4 | AUC 0.8423359274711169\n",
      "SAVING BEST!\n",
      "1302/1302 05:53<00:00 , loss=0.0661, lr=0.000677, step=6510\n",
      "420/420 00:47<00:00 \n",
      "EPOCH 5 | AUC 0.8211952297617138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 05:53<00:00 , loss=0.0522, lr=0.00056, step=7812 \n",
      "420/420 00:48<00:00 \n",
      "EPOCH 6 | AUC 0.8557156067808622\n",
      "SAVING BEST!\n",
      "1302/1302 05:54<00:00 , loss=0.0439, lr=0.00044, step=9114 \n",
      "420/420 00:48<00:00 \n",
      "EPOCH 7 | AUC 0.8637233148133291\n",
      "SAVING BEST!\n",
      "1302/1302 05:54<00:00 , loss=0.0321, lr=0.000323, step=10416\n",
      "420/420 00:47<00:00 \n",
      "EPOCH 8 | AUC 0.8437416217038405\n",
      "1302/1302 05:53<00:00 , loss=0.0245, lr=0.000216, step=11718\n",
      "420/420 00:48<00:00 \n",
      "EPOCH 9 | AUC 0.8243307183020432\n",
      "1302/1302 05:53<00:00 , loss=0.0172, lr=0.000126, step=13020\n",
      "420/420 00:48<00:00 \n",
      "EPOCH 10 | AUC 0.843277409773481\n",
      "[rank0]:[W314 05:08:41.538100732 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0314 05:08:44.754688 2388588 site-packages/torch/distributed/run.py:792] \n",
      "W0314 05:08:44.754688 2388588 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0314 05:08:44.754688 2388588 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0314 05:08:44.754688 2388588 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "[rank1]:[W314 05:08:52.640798744 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "torch.Size([2, 1])\n",
      "[rank0]:[W314 05:08:53.165066018 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/1303 00:00<? [rank1]:[W314 05:08:55.488128953 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W314 05:08:55.522641271 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "1303/1303 06:01<00:00 , loss=0.183, lr=0.000985, step=1303\n",
      "425/425 00:50<00:00 \n",
      "EPOCH 1 | AUC 0.8602173954765875\n",
      "SAVING BEST!\n",
      "1303/1303 06:01<00:00 , loss=0.13, lr=0.000943, step=2606 \n",
      "425/425 00:49<00:00 \n",
      "EPOCH 2 | AUC 0.872504958011507\n",
      "SAVING BEST!\n",
      "1303/1303 06:00<00:00 , loss=0.103, lr=0.000874, step=3909\n",
      "425/425 00:49<00:00 \n",
      "EPOCH 3 | AUC 0.868736943903032\n",
      "1303/1303 06:00<00:00 , loss=0.0795, lr=0.000784, step=5212\n",
      "425/425 00:49<00:00 \n",
      "EPOCH 4 | AUC 0.878757586290725\n",
      "SAVING BEST!\n",
      "1303/1303 06:00<00:00 , loss=0.0667, lr=0.000677, step=6515\n",
      "425/425 00:49<00:00 \n",
      "EPOCH 5 | AUC 0.8841469661234533\n",
      "SAVING BEST!\n",
      "1303/1303 06:00<00:00 , loss=0.0535, lr=0.00056, step=7818 \n",
      "425/425 00:49<00:00 \n",
      "EPOCH 6 | AUC 0.8959982449705505\n",
      "SAVING BEST!\n",
      "1303/1303 05:59<00:00 , loss=0.0397, lr=0.00044, step=9121 \n",
      "425/425 00:49<00:00 \n",
      "EPOCH 7 | AUC 0.8708361872998136\n",
      "1303/1303 06:01<00:00 , loss=0.0284, lr=0.000323, step=10424\n",
      "425/425 00:49<00:00 \n",
      "EPOCH 8 | AUC 0.8648009118476215\n",
      "1303/1303 05:58<00:00 , loss=0.023, lr=0.000216, step=11727 \n",
      "425/425 00:49<00:00 \n",
      "EPOCH 9 | AUC 0.8698920113106637\n",
      "1303/1303 05:58<00:00 , loss=0.0153, lr=0.000126, step=13030\n",
      "425/425 00:49<00:00 \n",
      "EPOCH 10 | AUC 0.8646818449347249\n",
      "[rank0]:[W314 06:17:24.201609743 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0314 06:17:27.530615 2397523 site-packages/torch/distributed/run.py:792] \n",
      "W0314 06:17:27.530615 2397523 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0314 06:17:27.530615 2397523 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0314 06:17:27.530615 2397523 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "[rank0]:[W314 06:17:35.480071729 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "torch.Size([2, 1])\n",
      "[rank1]:[W314 06:17:37.956025497 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/1286 00:00<? [rank1]:[W314 06:17:39.198301317 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W314 06:17:39.216700744 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286/1286 05:51<00:00 , loss=0.195, lr=0.000985, step=1286\n",
      "413/413 00:48<00:00 \n",
      "EPOCH 1 | AUC 0.8206277757712495\n",
      "SAVING BEST!\n",
      "1286/1286 05:53<00:00 , loss=0.134, lr=0.000943, step=2572\n",
      "413/413 00:47<00:00 \n",
      "EPOCH 2 | AUC 0.8591668785507358\n",
      "SAVING BEST!\n",
      "1286/1286 05:51<00:00 , loss=0.108, lr=0.000874, step=3858\n",
      "413/413 00:47<00:00 \n",
      "EPOCH 3 | AUC 0.8416077548914417\n",
      "1286/1286 05:49<00:00 , loss=0.0854, lr=0.000784, step=5144\n",
      "413/413 00:47<00:00 \n",
      "EPOCH 4 | AUC 0.863288885743338\n",
      "SAVING BEST!\n",
      "1286/1286 05:51<00:00 , loss=0.069, lr=0.000677, step=6430 \n",
      "413/413 00:47<00:00 \n",
      "EPOCH 5 | AUC 0.8405179597124929\n",
      "1286/1286 05:51<00:00 , loss=0.0532, lr=0.00056, step=7716 \n",
      "413/413 00:47<00:00 \n",
      "EPOCH 6 | AUC 0.8497558423671783\n",
      "1286/1286 05:51<00:00 , loss=0.0403, lr=0.00044, step=9002 \n",
      "413/413 00:47<00:00 \n",
      "EPOCH 7 | AUC 0.8315900749920438\n",
      "160/1286 00:44<05:04 , loss=0.0302, lr=0.000425, step=9162^C\n",
      "W0314 07:05:04.813325 2397523 site-packages/torch/distributed/elastic/agent/server/api.py:719] Received Signals.SIGINT death signal, shutting down workers\n",
      "W0314 07:05:04.814641 2397523 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2397573 closing signal SIGINT\n",
      "W0314 07:05:04.814910 2397523 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2397574 closing signal SIGINT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#Adding this line solved my problem\n",
    "!kill $(ps aux | grep \"run.py\" | grep -v grep | awk '{print $2}')\n",
    "\n",
    "DIR = \"./data/classification_stage1/\"\n",
    "NAME = \"tf_efficientnet_b3.ns_jft_in1k\"\n",
    "V = \"1\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1.ipynb\n",
    "\n",
    "for F in range(0, 4):\n",
    "    \n",
    "    shutil.copy(\"train1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !CUDA_VISIBLE_DEVICES=1,2 python -m torch.distributed.launch --nproc_per_node=2 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f160b2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1b7f0a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n",
      "[NbConvertApp] Converting notebook train1_pseudo1.ipynb to script\n",
      "[NbConvertApp] Writing 40395 bytes to train1_pseudo1.py\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0317 04:56:41.793410 2928158 site-packages/torch/distributed/run.py:792] \n",
      "W0317 04:56:41.793410 2928158 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0317 04:56:41.793410 2928158 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0317 04:56:41.793410 2928158 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "torch.Size([2, 1])\n",
      "[rank0]:[W317 04:56:52.167224162 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank1]:[W317 04:56:52.286718893 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/2163 00:00<? [rank0]:[W317 04:56:54.686836525 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank1]:[W317 04:56:54.687211444 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "2163/2163 09:52<00:00 , loss=0.237, lr=0.000985, step=2163\n",
      "426/426 00:49<00:00 \n",
      "EPOCH 1 | AUC 0.887570822969751\n",
      "SAVING BEST!\n",
      "2163/2163 09:56<00:00 , loss=0.196, lr=0.000943, step=4326\n",
      "426/426 00:50<00:00 \n",
      "EPOCH 2 | AUC 0.8897961834104839\n",
      "SAVING BEST!\n",
      "2163/2163 09:56<00:00 , loss=0.182, lr=0.000874, step=6489\n",
      "426/426 00:50<00:00 \n",
      "EPOCH 3 | AUC 0.9003278863129424\n",
      "SAVING BEST!\n",
      "2163/2163 09:53<00:00 , loss=0.17, lr=0.000784, step=8652 \n",
      "426/426 00:50<00:00 \n",
      "EPOCH 4 | AUC 0.89802095859421\n",
      "2163/2163 09:54<00:00 , loss=0.163, lr=0.000677, step=10815\n",
      "426/426 00:49<00:00 \n",
      "EPOCH 5 | AUC 0.8950715824766236\n",
      "2163/2163 09:52<00:00 , loss=0.156, lr=0.00056, step=12978 \n",
      "426/426 00:50<00:00 \n",
      "EPOCH 6 | AUC 0.899091599841024\n",
      "2163/2163 09:52<00:00 , loss=0.146, lr=0.00044, step=15141 \n",
      "426/426 00:50<00:00 \n",
      "EPOCH 7 | AUC 0.8902948518010962\n",
      "2163/2163 09:52<00:00 , loss=0.141, lr=0.000323, step=17304\n",
      "426/426 00:49<00:00 \n",
      "EPOCH 8 | AUC 0.9010846467727847\n",
      "SAVING BEST!\n",
      "2163/2163 09:48<00:00 , loss=0.137, lr=0.000216, step=19467\n",
      "426/426 00:50<00:00 \n",
      "EPOCH 9 | AUC 0.8953899659659355\n",
      "2163/2163 09:53<00:00 , loss=0.132, lr=0.000126, step=21630\n",
      "426/426 00:49<00:00 \n",
      "EPOCH 10 | AUC 0.8911187048832467\n",
      "[rank0]:[W317 06:44:22.674272908 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0317 06:44:26.791200 2943057 site-packages/torch/distributed/run.py:792] \n",
      "W0317 06:44:26.791200 2943057 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0317 06:44:26.791200 2943057 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0317 06:44:26.791200 2943057 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "[rank0]:[W317 06:44:37.565233597 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "torch.Size([2, 1])\n",
      "[rank1]:[W317 06:44:39.064453811 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/2175 00:00<? [rank0]:[W317 06:44:41.444531280 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank1]:[W317 06:44:41.451631449 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "2175/2175 09:59<00:00 , loss=0.231, lr=0.000985, step=2175\n",
      "420/420 00:49<00:00 \n",
      "EPOCH 1 | AUC 0.8952562024463789\n",
      "SAVING BEST!\n",
      "2175/2175 10:01<00:00 , loss=0.196, lr=0.000943, step=4350\n",
      "420/420 00:50<00:00 \n",
      "EPOCH 2 | AUC 0.9028037727822796\n",
      "SAVING BEST!\n",
      "2175/2175 09:59<00:00 , loss=0.182, lr=0.000874, step=6525\n",
      "420/420 00:49<00:00 \n",
      "EPOCH 3 | AUC 0.8966127447047497\n",
      "2175/2175 09:58<00:00 , loss=0.169, lr=0.000784, step=8700\n",
      "420/420 00:48<00:00 \n",
      "EPOCH 4 | AUC 0.8766942126658108\n",
      "2175/2175 09:56<00:00 , loss=0.162, lr=0.000677, step=10875\n",
      "420/420 00:49<00:00 \n",
      "EPOCH 5 | AUC 0.8823359922409874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "987/2175 04:31<05:25 , loss=0.154, lr=0.000625, step=11862^C\n",
      "W0317 07:43:22.431414 2943057 site-packages/torch/distributed/elastic/agent/server/api.py:719] Received Signals.SIGINT death signal, shutting down workers\n",
      "W0317 07:43:22.432332 2943057 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2943123 closing signal SIGINT\n",
      "W0317 07:43:22.432597 2943057 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2943130 closing signal SIGINT\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0317 07:43:24.454656 2951971 site-packages/torch/distributed/run.py:792] \n",
      "W0317 07:43:24.454656 2951971 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0317 07:43:24.454656 2951971 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0317 07:43:24.454656 2951971 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "[rank1]:[W317 07:43:34.768314453 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "torch.Size([2, 1])\n",
      "[rank0]:[W317 07:43:35.215556981 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/2177 00:00<? [rank1]:[W317 07:43:37.195247931 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W317 07:43:37.215524191 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "9/2177 00:03<12:27 , loss=0.46, lr=0.001, step=9 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#Adding this line solved my problem\n",
    "!kill $(ps aux | grep \"run.py\" | grep -v grep | awk '{print $2}')\n",
    "\n",
    "DIR = \"./data/classification_stage1/\"\n",
    "NAME = \"tf_efficientnet_b3.ns_jft_in1k\"\n",
    "V = \"2\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1_pseudo1.ipynb\n",
    "\n",
    "for F in range(0, 4):\n",
    "    \n",
    "    shutil.copy(\"train1_pseudo1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !CUDA_VISIBLE_DEVICES=1,2 python -m torch.distributed.launch --nproc_per_node=2 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaa557e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78cbd659",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]\n",
      "[NbConvertApp] Converting notebook train1_pseudo1.ipynb to script\n",
      "[NbConvertApp] Writing 40396 bytes to train1_pseudo1.py\n",
      "/home/harshit/anaconda3/lib/python3.9/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use-env is set by default in torchrun.\n",
      "If your script expects `--local-rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  main()\n",
      "W0317 22:03:46.983401 3055787 site-packages/torch/distributed/run.py:792] \n",
      "W0317 22:03:46.983401 3055787 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0317 22:03:46.983401 3055787 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0317 22:03:46.983401 3055787 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "torch.Size([2, 1])\n",
      "torch.Size([2, 1])\n",
      "[rank0]:[W317 22:03:57.297905280 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[rank1]:[W317 22:03:57.363508068 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "0/2163 00:00<? [rank1]:[W317 22:04:00.083707722 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W317 22:04:00.083784059 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "2163/2163 10:04<00:00 , loss=0.257, lr=0.000985, step=2163\n",
      "426/426 00:52<00:00 \n",
      "EPOCH 1 | AUC 0.8613779905222123\n",
      "SAVING BEST!\n",
      "2163/2163 10:02<00:00 , loss=0.211, lr=0.000943, step=4326\n",
      "426/426 00:53<00:00 \n",
      "EPOCH 2 | AUC 0.8548337134344913\n",
      "2163/2163 10:02<00:00 , loss=0.196, lr=0.000874, step=6489\n",
      "426/426 00:52<00:00 \n",
      "EPOCH 3 | AUC 0.8838820489068441\n",
      "SAVING BEST!\n",
      "2163/2163 10:05<00:00 , loss=0.184, lr=0.000784, step=8652\n",
      "426/426 00:52<00:00 \n",
      "EPOCH 4 | AUC 0.8791058722287252\n",
      "2163/2163 10:03<00:00 , loss=0.175, lr=0.000677, step=10815\n",
      "426/426 00:52<00:00 \n",
      "EPOCH 5 | AUC 0.8941026999452674\n",
      "SAVING BEST!\n",
      "2163/2163 10:02<00:00 , loss=0.165, lr=0.00056, step=12978 \n",
      "426/426 00:52<00:00 \n",
      "EPOCH 6 | AUC 0.8999252009201262\n",
      "SAVING BEST!\n",
      "2163/2163 10:03<00:00 , loss=0.155, lr=0.00044, step=15141 \n",
      "426/426 00:53<00:00 \n",
      "EPOCH 7 | AUC 0.9022297947774358\n",
      "SAVING BEST!\n",
      "2163/2163 10:04<00:00 , loss=0.148, lr=0.000323, step=17304\n",
      "426/426 00:53<00:00 \n",
      "EPOCH 8 | AUC 0.8929855229161499\n",
      "2163/2163 10:06<00:00 , loss=0.141, lr=0.000216, step=19467\n",
      "426/426 00:53<00:00 \n",
      "EPOCH 9 | AUC 0.8909632555678921\n",
      "2163/2163 10:06<00:00 , loss=0.136, lr=0.000126, step=21630\n",
      "426/426 00:53<00:00 \n",
      "EPOCH 10 | AUC 0.8866210238810367\n",
      "[rank0]:[W317 23:53:51.974051548 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#Adding this line solved my problem\n",
    "!kill $(ps aux | grep \"run.py\" | grep -v grep | awk '{print $2}')\n",
    "\n",
    "DIR = \"./data/classification_stage1/\"\n",
    "NAME = \"tf_efficientnetv2_s.in21k_ft_in1k\"\n",
    "V = \"7\"\n",
    "\n",
    "os.makedirs(f\"{DIR}/{NAME}_v{V}/\", exist_ok=1)\n",
    "\n",
    "!jupyter nbconvert --to script train1_pseudo1.ipynb\n",
    "\n",
    "for F in range(0, 1):\n",
    "    \n",
    "    shutil.copy(\"train1_pseudo1.py\", f\"{DIR}/{NAME}_v{V}/run.py\")\n",
    "    \n",
    "    filedata = None\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'r') as file:\n",
    "        filedata = file.read()\n",
    "\n",
    "    # Replace the target string\n",
    "    filedata = filedata.replace('FOLD = 0', f'FOLD = {F}')\n",
    "    filedata = filedata.replace('model_name = -1', f\"model_name = '{NAME}'\")\n",
    "    filedata = filedata.replace(f'V = -1', f\"V = '{V}'\")\n",
    "    \n",
    "    # Write the file out again\n",
    "    with open(f\"{DIR}/{NAME}_v{V}/run.py\", 'w') as file:\n",
    "        file.write(filedata)\n",
    "        \n",
    "    #!python \"{DIR}/{NAME}_v{V}/run.py\"\n",
    "    \n",
    "    !CUDA_VISIBLE_DEVICES=1,2 python -m torch.distributed.launch --nproc_per_node=2 \"{DIR}/{NAME}_v{V}/run.py\" | tee \"{DIR}/{NAME}_v{V}/log_{F}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de7e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de38c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b2dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70abe02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee659c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9741c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e5db03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eac1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df6b5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29c20b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.8570414638299453,\n",
       "  0.8355978656029364,\n",
       "  0.8725100652964783,\n",
       "  0.8579576537631352],\n",
       " 0.8557767621231238)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "scs = [np.array([float(line.split('| AUC ')[1].split(' |')[0]) for line in open(f'./data/classification_stage1/tf_efficientnetv2_s.in21k_ft_in1k_v4/log_{F}.txt').readlines() if 'AUC' in line]).max() for F in range(4)]\n",
    "scs, np.mean(scs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca43144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cfcd8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.8675850364215577,\n",
       "  0.8683378860284285,\n",
       "  0.8828426572382704,\n",
       "  0.8761409572079395],\n",
       " 0.873726634224049)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "scs = [np.array([float(line.split('| AUC ')[1].split(' |')[0]) for line in open(f'./data/classification_stage1/tf_efficientnetv2_s.in21k_ft_in1k_v5/log_{F}.txt').readlines() if 'AUC' in line]).max() for F in range(4)]\n",
    "scs, np.mean(scs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b74ee96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5dbd5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.8718704774804651,\n",
       "  0.8723636886733793,\n",
       "  0.8867102367477151,\n",
       "  0.8886824850099955],\n",
       " 0.8799067219778888)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "scs = [np.array([float(line.split('| AUC ')[1].split(' |')[0]) for line in open(f'./data/classification_stage1/tf_efficientnetv2_s.in21k_ft_in1k_v6/log_{F}.txt').readlines() if 'AUC' in line]).max() for F in range(4)]\n",
    "scs, np.mean(scs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d557d273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1122f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.8837104865179308,\n",
       "  0.8637233148133291,\n",
       "  0.8959982449705505,\n",
       "  0.863288885743338],\n",
       " 0.8766802330112872)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "scs = [np.array([float(line.split('| AUC ')[1].split(' |')[0]) for line in open(f'./data/classification_stage1/tf_efficientnet_b3.ns_jft_in1k_v1/log_{F}.txt').readlines() if 'AUC' in line]).max() for F in range(4)]\n",
    "scs, np.mean(scs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b21aa5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f63664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd289e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfd0914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b459ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
